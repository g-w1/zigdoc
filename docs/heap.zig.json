[{"pl":"pub const FixedBufferAllocator = struct ","src":661,"more_decls":{"pl":"allocator: Allocator","src":662,"more_decls":null}{"pl":"end_index: usize","src":663,"more_decls":null}{"pl":"buffer: []u8","src":664,"more_decls":null}{"pl":"fn init(buffer: []u8) FixedBufferAllocator","src":666,"more_decls":null}{"pl":"pub fn ownsPtr(self: *FixedBufferAllocator, ptr: [*]u8) bool {\n    return sliceContainsPtr(self.buffer, ptr);\n}","src":677,"more_decls":null}{"pl":"pub fn ownsSlice(self: *FixedBufferAllocator, slice: []u8) bool {\n    return sliceContainsSlice(self.buffer, slice);\n}","src":681,"more_decls":null}{"doc_comment":"NOTE: this will not work in all cases, if the last allocation had an adjusted_index\nthen we won't be able to determine what the last allocation was.  This is because\nthe alignForward operation done in alloc is not reverisible.","pl":"pub fn isLastAllocation(self: *FixedBufferAllocator, buf: []u8) bool {\n    return buf.ptr + buf.len == self.buffer.ptr + self.end_index;\n}","src":688,"more_decls":null}{"pl":"pub fn reset(self: *FixedBufferAllocator) void {\n    self.end_index = 0;\n}","src":737,"more_decls":null}}{"doc_comment":"Verifies that the adjusted length will still map to the full length","pl":"pub fn alignPageAllocLen(full_len: usize, len: usize, len_align: u29) usize {\n    const aligned_len = mem.alignAllocLen(full_len, len, len_align);\n    assert(mem.alignForward(aligned_len, mem.page_size) == full_len);\n    return aligned_len;\n}","src":219,"more_decls":null}{"pl":"fn stackFallback(comptime size: usize, fallback_allocator: *Allocator) StackFallbackAllocator(size)","src":785,"more_decls":null}{"pl":"fn StackFallbackAllocator(comptime size: usize) type","sub_container_type":"struct ","src":797,"more_decls":{"pl":"buffer: [size]u8","src":801,"more_decls":null}{"pl":"allocator: Allocator","src":802,"more_decls":null}{"pl":"fallback_allocator: *Allocator","src":803,"more_decls":null}{"pl":"fixed_buffer_allocator: FixedBufferAllocator","src":804,"more_decls":null}{"pl":"pub fn get(self: *Self) *Allocator {\n    self.fixed_buffer_allocator = FixedBufferAllocator.init(self.buffer[0..]);\n    return &self.allocator;\n}","src":806,"more_decls":null}}{"doc_comment":"This one should not try alignments that exceed what C malloc can handle.","pl":"fn testAllocator(base_allocator: *mem.Allocator) !void","src":1021,"more_decls":null}{"pl":"fn testAllocatorAligned(base_allocator: *mem.Allocator) !void","src":1067,"more_decls":null}{"pl":"fn testAllocatorLargeAlignment(base_allocator: *mem.Allocator) mem.Allocator.Error!void","src":1097,"more_decls":null}{"pl":"fn testAllocatorAlignedShrink(base_allocator: *mem.Allocator) mem.Allocator.Error!void","src":1129,"more_decls":null}{"pl":"const LoggingAllocator = @import(\"heap/logging_allocator.zig\").LoggingAllocator","src":16,"more_decls":null}{"pl":"const loggingAllocator = @import(\"heap/logging_allocator.zig\").loggingAllocator","src":17,"more_decls":null}{"pl":"const ArenaAllocator = @import(\"heap/arena_allocator.zig\").ArenaAllocator","src":18,"more_decls":null}{"pl":"const GeneralPurposeAllocator = @import(\"heap/general_purpose_allocator.zig\").GeneralPurposeAllocator","src":19,"more_decls":null}{"doc_comment":"Supports the full Allocator interface, including alignment, and exploiting\n`malloc_usable_size` if available. For an allocator that directly calls\n`malloc`/`free`, see `raw_c_allocator`.","pl":"const c_allocator = &c_allocator_state","src":153,"more_decls":null}{"doc_comment":"Asserts allocations are within `@alignOf(std.c.max_align_t)` and directly calls\n`malloc`/`free`. Does not attempt to utilize `malloc_usable_size`.\nThis allocator is safe to use as the backing allocator with\n`ArenaAllocator` for example and is more optimal in such a case\nthan `c_allocator`.","pl":"const raw_c_allocator = &raw_c_allocator_state","src":164,"more_decls":null}{"doc_comment":"This allocator makes a syscall directly for every allocation and free.\nThread-safe and lock-free.","pl":"const page_allocator = if (std.Target.current.isWasm())\n    &wasm_page_allocator_state\nelse if (std.Target.current.os.tag == .freestanding)\n    root.os.heap.page_allocator\nelse\n    &page_allocator_state","src":202,"more_decls":null}{"doc_comment":"TODO Utilize this on Windows.","pl":"var next_mmap_addr_hint: ?[*]align(mem.page_size) u8 = null","src":226,"more_decls":null}{"pl":"const HeapAllocator = switch (builtin.os.tag) {\n    .windows => struct {\n        allocator: Allocator,\n        heap_handle: ?HeapHandle,\n\n        const HeapHandle = os.windows.HANDLE;\n\n        pub fn init() HeapAllocator {\n            return HeapAllocator{\n                .allocator = Allocator{\n                    .allocFn = alloc,\n                    .resizeFn = resize,\n                },\n                .heap_handle = null,\n            };\n        }\n\n        pub fn deinit(self: *HeapAllocator) void {\n            if (self.heap_handle) |heap_handle| {\n                os.windows.HeapDestroy(heap_handle);\n            }\n        }\n\n        fn getRecordPtr(buf: []u8) *align(1) usize {\n            return @intToPtr(*align(1) usize, @ptrToInt(buf.ptr) + buf.len);\n        }\n\n        fn alloc(\n            allocator: *Allocator,\n            n: usize,\n            ptr_align: u29,\n            len_align: u29,\n            return_address: usize,\n        ) error{OutOfMemory}![]u8 {\n            const self = @fieldParentPtr(HeapAllocator, \"allocator\", allocator);\n\n            const amt = n + ptr_align - 1 + @sizeOf(usize);\n            const optional_heap_handle = @atomicLoad(?HeapHandle, &self.heap_handle, builtin.AtomicOrder.SeqCst);\n            const heap_handle = optional_heap_handle orelse blk: {\n                const options = if (builtin.single_threaded) os.windows.HEAP_NO_SERIALIZE else 0;\n                const hh = os.windows.kernel32.HeapCreate(options, amt, 0) orelse return error.OutOfMemory;\n                const other_hh = @cmpxchgStrong(?HeapHandle, &self.heap_handle, null, hh, builtin.AtomicOrder.SeqCst, builtin.AtomicOrder.SeqCst) orelse break :blk hh;\n                os.windows.HeapDestroy(hh);\n                break :blk other_hh.?; // can't be null because of the cmpxchg\n            };\n            const ptr = os.windows.kernel32.HeapAlloc(heap_handle, 0, amt) orelse return error.OutOfMemory;\n            const root_addr = @ptrToInt(ptr);\n            const aligned_addr = mem.alignForward(root_addr, ptr_align);\n            const return_len = init: {\n                if (len_align == 0) break :init n;\n                const full_len = os.windows.kernel32.HeapSize(heap_handle, 0, ptr);\n                assert(full_len != std.math.maxInt(usize));\n                assert(full_len >= amt);\n                break :init mem.alignBackwardAnyAlign(full_len - (aligned_addr - root_addr) - @sizeOf(usize), len_align);\n            };\n            const buf = @intToPtr([*]u8, aligned_addr)[0..return_len];\n            getRecordPtr(buf).* = root_addr;\n            return buf;\n        }\n\n        fn resize(\n            allocator: *Allocator,\n            buf: []u8,\n            buf_align: u29,\n            new_size: usize,\n            len_align: u29,\n            return_address: usize,\n        ) error{OutOfMemory}!usize {\n            const self = @fieldParentPtr(HeapAllocator, \"allocator\", allocator);\n            if (new_size == 0) {\n                os.windows.HeapFree(self.heap_handle.?, 0, @intToPtr(*c_void, getRecordPtr(buf).*));\n                return 0;\n            }\n\n            const root_addr = getRecordPtr(buf).*;\n            const align_offset = @ptrToInt(buf.ptr) - root_addr;\n            const amt = align_offset + new_size + @sizeOf(usize);\n            const new_ptr = os.windows.kernel32.HeapReAlloc(\n                self.heap_handle.?,\n                os.windows.HEAP_REALLOC_IN_PLACE_ONLY,\n                @intToPtr(*c_void, root_addr),\n                amt,\n            ) orelse return error.OutOfMemory;\n            assert(new_ptr == @intToPtr(*c_void, root_addr));\n            const return_len = init: {\n                if (len_align == 0) break :init new_size;\n                const full_len = os.windows.kernel32.HeapSize(self.heap_handle.?, 0, new_ptr);\n                assert(full_len != std.math.maxInt(usize));\n                assert(full_len >= amt);\n                break :init mem.alignBackwardAnyAlign(full_len - align_offset, len_align);\n            };\n            getRecordPtr(buf.ptr[0..return_len]).* = root_addr;\n            return return_len;\n        }\n    },\n    else => @compileError(\"Unsupported OS\"),\n}","src":553,"more_decls":null}{"pl":"const ThreadSafeFixedBufferAllocator = blk: {\n    if (builtin.single_threaded) {\n        break :blk FixedBufferAllocator;\n    } else {\n        // lock free\n        break :blk struct {\n            allocator: Allocator,\n            end_index: usize,\n            buffer: []u8,\n\n            pub fn init(buffer: []u8) ThreadSafeFixedBufferAllocator {\n                return ThreadSafeFixedBufferAllocator{\n                    .allocator = Allocator{\n                        .allocFn = alloc,\n                        .resizeFn = Allocator.noResize,\n                    },\n                    .buffer = buffer,\n                    .end_index = 0,\n                };\n            }\n\n            fn alloc(allocator: *Allocator, n: usize, ptr_align: u29, len_align: u29, ra: usize) ![]u8 {\n                const self = @fieldParentPtr(ThreadSafeFixedBufferAllocator, \"allocator\", allocator);\n                var end_index = @atomicLoad(usize, &self.end_index, builtin.AtomicOrder.SeqCst);\n                while (true) {\n                    const addr = @ptrToInt(self.buffer.ptr) + end_index;\n                    const adjusted_addr = mem.alignForward(addr, ptr_align);\n                    const adjusted_index = end_index + (adjusted_addr - addr);\n                    const new_end_index = adjusted_index + n;\n                    if (new_end_index > self.buffer.len) {\n                        return error.OutOfMemory;\n                    }\n                    end_index = @cmpxchgWeak(usize, &self.end_index, end_index, new_end_index, builtin.AtomicOrder.SeqCst, builtin.AtomicOrder.SeqCst) orelse return self.buffer[adjusted_index..new_end_index];\n                }\n            }\n\n            pub fn reset(self: *ThreadSafeFixedBufferAllocator) void {\n                self.end_index = 0;\n            }\n        };\n    }\n}","src":742,"more_decls":null}]