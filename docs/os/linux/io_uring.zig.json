[{"pl":"pub const IO_Uring = struct ","src":18,"more_decls":{"pl":"fd: os.fd_t = -1","src":19,"more_decls":null}{"pl":"sq: SubmissionQueue","src":20,"more_decls":null}{"pl":"cq: CompletionQueue","src":21,"more_decls":null}{"pl":"flags: u32","src":22,"more_decls":null}{"pl":"features: u32","src":23,"more_decls":null}{"doc_comment":"A friendly way to setup an io_uring, with default io_uring_params.\n`entries` must be a power of two between 1 and 4096, although the kernel will make the final\ncall on how many entries the submission and completion queues will ultimately have,\nsee https://github.com/torvalds/linux/blob/v5.8/fs/io_uring.c#L8027-L8050.\nMatches the interface of io_uring_queue_init() in liburing.","pl":"fn init(entries: u13, flags: u32) !IO_Uring","src":30,"more_decls":null}{"doc_comment":"A powerful way to setup an io_uring, if you want to tweak io_uring_params such as submission\nqueue thread cpu affinity or thread idle timeout (the kernel and our default is 1 second).\n`params` is passed by reference because the kernel needs to modify the parameters.\nMatches the interface of io_uring_queue_init_params() in liburing.","pl":"fn init_params(entries: u13, p: *io_uring_params) !IO_Uring","src":42,"more_decls":null}{"pl":"fn deinit(self: *IO_Uring) void","src":126,"more_decls":null}{"doc_comment":"Returns a pointer to a vacant SQE, or an error if the submission queue is full.\nWe follow the implementation (and atomics) of liburing's `io_uring_get_sqe()` exactly.\nHowever, instead of a null we return an error to force safe handling.\nAny situation where the submission queue is full tends more towards a control flow error,\nand the null return in liburing is more a C idiom than anything else, for lack of a better\nalternative. In Zig, we have first-class error handling... so let's use it.\nMatches the implementation of io_uring_get_sqe() in liburing.","pl":"fn get_sqe(self: *IO_Uring) !*io_uring_sqe","src":142,"more_decls":null}{"doc_comment":"Submits the SQEs acquired via get_sqe() to the kernel. You can call this once after you have\ncalled get_sqe() multiple times to setup multiple I/O requests.\nReturns the number of SQEs submitted.\nMatches the implementation of io_uring_submit() in liburing.","pl":"pub fn submit(self: *IO_Uring) !u32 {\n    return self.submit_and_wait(0);\n}","src":157,"more_decls":null}{"doc_comment":"Like submit(), but allows waiting for events as well.\nReturns the number of SQEs submitted.\nMatches the implementation of io_uring_submit_and_wait() in liburing.","pl":"fn submit_and_wait(self: *IO_Uring, wait_nr: u32) !u32","src":164,"more_decls":null}{"doc_comment":"Tell the kernel we have submitted SQEs and/or want to wait for CQEs.\nReturns the number of SQEs submitted.","pl":"fn enter(self: *IO_Uring, to_submit: u32, min_complete: u32, flags: u32) !u32","src":178,"more_decls":null}{"doc_comment":"Sync internal state with kernel ring state on the SQ side.\nReturns the number of all pending events in the SQ ring, for the shared ring.\nThis return value includes previously flushed SQEs, as per liburing.\nThe rationale is to suggest that an io_uring_enter() call is needed rather than not.\nMatches the implementation of __io_uring_flush_sq() in liburing.","pl":"fn flush_sq(self: *IO_Uring) u32","src":217,"more_decls":null}{"doc_comment":"Returns true if we are not using an SQ thread (thus nobody submits but us),\nor if IORING_SQ_NEED_WAKEUP is set and the SQ thread must be explicitly awakened.\nFor the latter case, we set the SQ thread wakeup flag.\nMatches the implementation of sq_ring_needs_enter() in liburing.","pl":"fn sq_ring_needs_enter(self: *IO_Uring, flags: *u32) bool","src":238,"more_decls":null}{"doc_comment":"Returns the number of flushed and unflushed SQEs pending in the submission queue.\nIn other words, this is the number of SQEs in the submission queue, i.e. its length.\nThese are SQEs that the kernel is yet to consume.\nMatches the implementation of io_uring_sq_ready in liburing.","pl":"pub fn sq_ready(self: *IO_Uring) u32 {\n    // Always use the shared ring state (i.e. head and not sqe_head) to avoid going out of sync,\n    // see https://github.com/axboe/liburing/issues/92.\n    return self.sq.sqe_tail -% @atomicLoad(u32, self.sq.head, .Acquire);\n}","src":252,"more_decls":null}{"doc_comment":"Returns the number of CQEs in the completion queue, i.e. its length.\nThese are CQEs that the application is yet to consume.\nMatches the implementation of io_uring_cq_ready in liburing.","pl":"pub fn cq_ready(self: *IO_Uring) u32 {\n    return @atomicLoad(u32, self.cq.tail, .Acquire) -% self.cq.head.*;\n}","src":261,"more_decls":null}{"doc_comment":"Copies as many CQEs as are ready, and that can fit into the destination `cqes` slice.\nIf none are available, enters into the kernel to wait for at most `wait_nr` CQEs.\nReturns the number of CQEs copied, advancing the CQ ring.\nProvides all the wait/peek methods found in liburing, but with batching and a single method.\nThe rationale for copying CQEs rather than copying pointers is that pointers are 8 bytes\nwhereas CQEs are not much more at only 16 bytes, and this provides a safer faster interface.\nSafer, because you no longer need to call cqe_seen(), avoiding idempotency bugs.\nFaster, because we can now amortize the atomic store release to `cq.head` across the batch.\nSee https://github.com/axboe/liburing/issues/103#issuecomment-686665007.\nMatches the implementation of io_uring_peek_batch_cqe() in liburing, but supports waiting.","pl":"fn copy_cqes(self: *IO_Uring, cqes: []io_uring_cqe, wait_nr: u32) !u32","src":275,"more_decls":null}{"doc_comment":"Returns a copy of an I/O completion, waiting for it if necessary, and advancing the CQ ring.\nA convenience method for `copy_cqes()` for when you don't need to batch or peek.","pl":"pub fn copy_cqe(ring: *IO_Uring) !io_uring_cqe {\n    var cqes: [1]io_uring_cqe = undefined;\n    const count = try ring.copy_cqes(&cqes, 1);\n    assert(count == 1);\n    return cqes[0];\n}","src":304,"more_decls":null}{"doc_comment":"Matches the implementation of cq_ring_needs_flush() in liburing.","pl":"pub fn cq_ring_needs_flush(self: *IO_Uring) bool {\n    return (@atomicLoad(u32, self.sq.flags, .Unordered) & linux.IORING_SQ_CQ_OVERFLOW) != 0;\n}","src":312,"more_decls":null}{"doc_comment":"For advanced use cases only that implement custom completion queue methods.\nIf you use copy_cqes() or copy_cqe() you must not call cqe_seen() or cq_advance().\nMust be called exactly once after a zero-copy CQE has been processed by your application.\nNot idempotent, calling more than once will result in other CQEs being lost.\nMatches the implementation of cqe_seen() in liburing.","pl":"pub fn cqe_seen(self: *IO_Uring, cqe: *io_uring_cqe) void {\n    self.cq_advance(1);\n}","src":321,"more_decls":null}{"doc_comment":"For advanced use cases only that implement custom completion queue methods.\nMatches the implementation of cq_advance() in liburing.","pl":"pub fn cq_advance(self: *IO_Uring, count: u32) void {\n    if (count > 0) {\n        // Ensure the kernel only sees the new head value after the CQEs have been read.\n        @atomicStore(u32, self.cq.head, self.cq.head.* +% count, .Release);\n    }\n}","src":327,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform an `fsync(2)`.\nReturns a pointer to the SQE so that you can further modify the SQE for advanced use cases.\nFor example, for `fdatasync()` you can set `IORING_FSYNC_DATASYNC` in the SQE's `rw_flags`.\nN.B. While SQEs are initiated in the order in which they appear in the submission queue,\noperations execute in parallel and completions are unordered. Therefore, an application that\nsubmits a write followed by an fsync in the submission queue cannot expect the fsync to\napply to the write, since the fsync may complete before the write is issued to the disk.\nYou should preferably use `link_with_next_sqe()` on a write's SQE to link it with an fsync,\nor else insert a full write barrier using `drain_previous_sqes()` when queueing an fsync.","pl":"pub fn fsync(self: *IO_Uring, user_data: u64, fd: os.fd_t, flags: u32) !*io_uring_sqe {\n    const sqe = try self.get_sqe();\n    io_uring_prep_fsync(sqe, fd, flags);\n    sqe.user_data = user_data;\n    return sqe;\n}","src":343,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a no-op.\nReturns a pointer to the SQE so that you can further modify the SQE for advanced use cases.\nA no-op is more useful than may appear at first glance.\nFor example, you could call `drain_previous_sqes()` on the returned SQE, to use the no-op to\nknow when the ring is idle before acting on a kill signal.","pl":"pub fn nop(self: *IO_Uring, user_data: u64) !*io_uring_sqe {\n    const sqe = try self.get_sqe();\n    io_uring_prep_nop(sqe);\n    sqe.user_data = user_data;\n    return sqe;\n}","src":355,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `read(2)`.\nReturns a pointer to the SQE.","pl":"fn read(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    buffer: []u8,\n    offset: u64,\n) !*io_uring_sqe","src":364,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `write(2)`.\nReturns a pointer to the SQE.","pl":"fn write(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    buffer: []const u8,\n    offset: u64,\n) !*io_uring_sqe","src":379,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `preadv()`.\nReturns a pointer to the SQE so that you can further modify the SQE for advanced use cases.\nFor example, if you want to do a `preadv2()` then set `rw_flags` on the returned SQE.\nSee https://linux.die.net/man/2/preadv.","pl":"fn readv(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    iovecs: []const os.iovec,\n    offset: u64,\n) !*io_uring_sqe","src":396,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `pwritev()`.\nReturns a pointer to the SQE so that you can further modify the SQE for advanced use cases.\nFor example, if you want to do a `pwritev2()` then set `rw_flags` on the returned SQE.\nSee https://linux.die.net/man/2/pwritev.","pl":"fn writev(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    iovecs: []const os.iovec_const,\n    offset: u64,\n) !*io_uring_sqe","src":413,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform an `accept4(2)` on a socket.\nReturns a pointer to the SQE.","pl":"fn accept(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    addr: *os.sockaddr,\n    addrlen: *os.socklen_t,\n    flags: u32,\n) !*io_uring_sqe","src":428,"more_decls":null}{"doc_comment":"Queue (but does not submit) an SQE to perform a `connect(2)` on a socket.\nReturns a pointer to the SQE.","pl":"fn connect(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    addr: *const os.sockaddr,\n    addrlen: os.socklen_t,\n) !*io_uring_sqe","src":444,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `recv(2)`.\nReturns a pointer to the SQE.","pl":"fn recv(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    buffer: []u8,\n    flags: u32,\n) !*io_uring_sqe","src":459,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `send(2)`.\nReturns a pointer to the SQE.","pl":"fn send(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    buffer: []const u8,\n    flags: u32,\n) !*io_uring_sqe","src":474,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform an `openat(2)`.\nReturns a pointer to the SQE.","pl":"fn openat(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    path: [*:0]const u8,\n    flags: u32,\n    mode: os.mode_t,\n) !*io_uring_sqe","src":489,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform a `close(2)`.\nReturns a pointer to the SQE.","pl":"pub fn close(self: *IO_Uring, user_data: u64, fd: os.fd_t) !*io_uring_sqe {\n    const sqe = try self.get_sqe();\n    io_uring_prep_close(sqe, fd);\n    sqe.user_data = user_data;\n    return sqe;\n}","src":505,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to register a timeout operation.\nReturns a pointer to the SQE.\n\nThe timeout will complete when either the timeout expires, or after the specified number of\nevents complete (if `count` is greater than `0`).\n\n`flags` may be `0` for a relative timeout, or `IORING_TIMEOUT_ABS` for an absolute timeout.\n\nThe completion event result will be `-ETIME` if the timeout completed through expiration,\n`0` if the timeout completed after the specified number of events, or `-ECANCELED` if the\ntimeout was removed before it expired.\n\nio_uring timeouts use the `CLOCK_MONOTONIC` clock source.","pl":"fn timeout(\n    self: *IO_Uring,\n    user_data: u64,\n    ts: *const os.__kernel_timespec,\n    count: u32,\n    flags: u32,\n) !*io_uring_sqe","src":525,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to remove an existing timeout operation.\nReturns a pointer to the SQE.\n\nThe timeout is identified by its `user_data`.\n\nThe completion event result will be `0` if the timeout was found and cancelled successfully,\n`-EBUSY` if the timeout was found but expiration was already in progress, or\n`-ENOENT` if the timeout was not found.","pl":"fn timeout_remove(\n    self: *IO_Uring,\n    user_data: u64,\n    timeout_user_data: u64,\n    flags: u32,\n) !*io_uring_sqe","src":546,"more_decls":null}{"doc_comment":"Queues (but does not submit) an SQE to perform an `fallocate(2)`.\nReturns a pointer to the SQE.","pl":"fn fallocate(\n    self: *IO_Uring,\n    user_data: u64,\n    fd: os.fd_t,\n    mode: i32,\n    offset: u64,\n    len: u64,\n) !*io_uring_sqe","src":560,"more_decls":null}{"doc_comment":"Registers an array of file descriptors.\nEvery time a file descriptor is put in an SQE and submitted to the kernel, the kernel must\nretrieve a reference to the file, and once I/O has completed the file reference must be\ndropped. The atomic nature of this file reference can be a slowdown for high IOPS workloads.\nThis slowdown can be avoided by pre-registering file descriptors.\nTo refer to a registered file descriptor, IOSQE_FIXED_FILE must be set in the SQE's flags,\nand the SQE's fd must be set to the index of the file descriptor in the registered array.\nRegistering file descriptors will wait for the ring to idle.\nFiles are automatically unregistered by the kernel when the ring is torn down.\nAn application need unregister only if it wants to register a new array of file descriptors.","pl":"fn register_files(self: *IO_Uring, fds: []const os.fd_t) !void","src":584,"more_decls":null}{"doc_comment":"Unregisters all registered file descriptors previously associated with the ring.","pl":"fn unregister_files(self: *IO_Uring) !void","src":615,"more_decls":null}}{"pl":"pub const SubmissionQueue = struct ","src":626,"more_decls":{"pl":"head: *u32","src":627,"more_decls":null}{"pl":"tail: *u32","src":628,"more_decls":null}{"pl":"mask: u32","src":629,"more_decls":null}{"pl":"flags: *u32","src":630,"more_decls":null}{"pl":"dropped: *u32","src":631,"more_decls":null}{"pl":"array: []u32","src":632,"more_decls":null}{"pl":"sqes: []io_uring_sqe","src":633,"more_decls":null}{"pl":"mmap: []align(mem.page_size) u8","src":634,"more_decls":null}{"pl":"mmap_sqes: []align(mem.page_size) u8","src":635,"more_decls":null}{"pl":"sqe_head: u32 = 0","src":641,"more_decls":null}{"pl":"sqe_tail: u32 = 0","src":642,"more_decls":null}{"pl":"fn init(fd: os.fd_t, p: io_uring_params) !SubmissionQueue","src":644,"more_decls":null}{"pl":"pub fn deinit(self: *SubmissionQueue) void {\n    os.munmap(self.mmap_sqes);\n    os.munmap(self.mmap);\n}","src":697,"more_decls":null}}{"pl":"pub const CompletionQueue = struct ","src":703,"more_decls":{"pl":"head: *u32","src":704,"more_decls":null}{"pl":"tail: *u32","src":705,"more_decls":null}{"pl":"mask: u32","src":706,"more_decls":null}{"pl":"overflow: *u32","src":707,"more_decls":null}{"pl":"cqes: []io_uring_cqe","src":708,"more_decls":null}{"pl":"fn init(fd: os.fd_t, p: io_uring_params, sq: SubmissionQueue) !CompletionQueue","src":710,"more_decls":null}{"pl":"pub fn deinit(self: *CompletionQueue) void {\n    // A no-op since we now share the mmap with the submission queue.\n    // Here for symmetry with the submission queue, and for any future feature support.\n}","src":729,"more_decls":null}}{"pl":"fn io_uring_prep_nop(sqe: *io_uring_sqe) void","src":735,"more_decls":null}{"pl":"fn io_uring_prep_fsync(sqe: *io_uring_sqe, fd: os.fd_t, flags: u32) void","src":753,"more_decls":null}{"pl":"fn io_uring_prep_rw(\n    op: linux.IORING_OP,\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    addr: anytype,\n    len: usize,\n    offset: u64,\n) void","src":771,"more_decls":null}{"pl":"pub fn io_uring_prep_read(sqe: *io_uring_sqe, fd: os.fd_t, buffer: []u8, offset: u64) void {\n    io_uring_prep_rw(.READ, sqe, fd, buffer.ptr, buffer.len, offset);\n}","src":796,"more_decls":null}{"pl":"pub fn io_uring_prep_write(sqe: *io_uring_sqe, fd: os.fd_t, buffer: []const u8, offset: u64) void {\n    io_uring_prep_rw(.WRITE, sqe, fd, buffer.ptr, buffer.len, offset);\n}","src":800,"more_decls":null}{"pl":"fn io_uring_prep_readv(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    iovecs: []const os.iovec,\n    offset: u64,\n) void","src":804,"more_decls":null}{"pl":"fn io_uring_prep_writev(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    iovecs: []const os.iovec_const,\n    offset: u64,\n) void","src":813,"more_decls":null}{"pl":"fn io_uring_prep_accept(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    addr: *os.sockaddr,\n    addrlen: *os.socklen_t,\n    flags: u32,\n) void","src":822,"more_decls":null}{"pl":"fn io_uring_prep_connect(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    addr: *const os.sockaddr,\n    addrlen: os.socklen_t,\n) void","src":835,"more_decls":null}{"pl":"pub fn io_uring_prep_recv(sqe: *io_uring_sqe, fd: os.fd_t, buffer: []u8, flags: u32) void {\n    io_uring_prep_rw(.RECV, sqe, fd, buffer.ptr, buffer.len, 0);\n    sqe.rw_flags = flags;\n}","src":845,"more_decls":null}{"pl":"pub fn io_uring_prep_send(sqe: *io_uring_sqe, fd: os.fd_t, buffer: []const u8, flags: u32) void {\n    io_uring_prep_rw(.SEND, sqe, fd, buffer.ptr, buffer.len, 0);\n    sqe.rw_flags = flags;\n}","src":850,"more_decls":null}{"pl":"fn io_uring_prep_openat(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    path: [*:0]const u8,\n    flags: u32,\n    mode: os.mode_t,\n) void","src":855,"more_decls":null}{"pl":"fn io_uring_prep_close(sqe: *io_uring_sqe, fd: os.fd_t) void","src":866,"more_decls":null}{"pl":"fn io_uring_prep_timeout(\n    sqe: *io_uring_sqe,\n    ts: *const os.__kernel_timespec,\n    count: u32,\n    flags: u32,\n) void","src":884,"more_decls":null}{"pl":"fn io_uring_prep_timeout_remove(sqe: *io_uring_sqe, timeout_user_data: u64, flags: u32) void","src":894,"more_decls":null}{"pl":"fn io_uring_prep_fallocate(\n    sqe: *io_uring_sqe,\n    fd: os.fd_t,\n    mode: i32,\n    offset: u64,\n    len: u64,\n) void","src":912,"more_decls":null}]